{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "RANDOM_VARIABLE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sitting-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('income.csv')\n",
    "data_test = pd.read_csv('income_test.csv')\n",
    "\n",
    "# Convert target variable to 0 and 1\n",
    "data[\"income\"] = data[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "data_test[\"income\"] = data_test[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "\n",
    "# Create X and y\n",
    "X_train = data.drop([\"income\"],axis=1)\n",
    "y_train = data['income'].values\n",
    "X_test = data_test.drop([\"income\"],axis=1)\n",
    "y_test = data_test['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-bouquet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       30725 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education_num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      30718 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      "dtypes: int64(6), object(7)\n",
      "memory usage: 3.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15315 entries, 0 to 15314\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             15315 non-null  int64 \n",
      " 1   workclass       15315 non-null  object\n",
      " 2   fnlwgt          15315 non-null  int64 \n",
      " 3   education       15315 non-null  object\n",
      " 4   education_num   15315 non-null  int64 \n",
      " 5   marital-status  15315 non-null  object\n",
      " 6   occupation      15315 non-null  object\n",
      " 7   relationship    15315 non-null  object\n",
      " 8   race            15315 non-null  object\n",
      " 9   sex             15315 non-null  object\n",
      " 10  capital-gain    15315 non-null  int64 \n",
      " 11  capital-loss    15315 non-null  int64 \n",
      " 12  hours-per-week  15315 non-null  int64 \n",
      "dtypes: int64(6), object(7)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "X_train.info()\n",
    "X_test.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "historic-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate attributes of type int and of type object\n",
    "int_X_train = X_train.select_dtypes(include=['int64']).copy()\n",
    "obj_X_train = X_train.select_dtypes(include=['object']).copy()\n",
    "\n",
    "int_X_test = X_test.select_dtypes(include=['int64']).copy()\n",
    "obj_X_test = X_test.select_dtypes(include=['object']).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fixed-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of missing values with a string \"missing\"\n",
    "imputer1 = SimpleImputer(strategy='constant', fill_value=\"missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automotive-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply simple imputation only in obj_X_train because this is where the missing values are\n",
    "obj_X_train = pd.DataFrame(data= imputer1.fit_transform(obj_X_train), index=obj_X_train.index, columns=obj_X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iraqi-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders to use\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "encoder = OneHotEncoder()\n",
    "oneHotEncoderFlag = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "satisfactory-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   workclass       30725 non-null  float64\n",
      " 1   education       32561 non-null  int32  \n",
      " 2   marital-status  32561 non-null  int32  \n",
      " 3   occupation      30718 non-null  float64\n",
      " 4   relationship    32561 non-null  int32  \n",
      " 5   race            32561 non-null  int32  \n",
      " 6   sex             32561 non-null  int32  \n",
      "dtypes: float64(2), int32(5)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Label Encode the categorical attributes and then turn the missing encoded values into NaN again\n",
    "missing = np.array([\"missing\"])\n",
    "for i in obj_X_train.columns:\n",
    "    flag = missing[0] in list(obj_X_train[i])  # flag indicates the presence of missing values in the column\n",
    "    obj_X_train[i] = labelEncoder.fit_transform(obj_X_train[i])\n",
    "    obj_X_test[i] = labelEncoder.transform(obj_X_test[i])\n",
    "    if flag: # search for encoded string missing and replace it with NaN\n",
    "        obj_X_train[i] = obj_X_train[i].replace(labelEncoder.transform(missing),np.nan)\n",
    "        \n",
    "obj_X_train.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accepted-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that all attributes are numerical, many imputation methods can be applied\n",
    "imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# imputer2 = KNNImputer(n_neighbors=10)\n",
    "# imputer2 = IterativeImputer(max_iter=15, random_state=RANDOM_VARIABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "academic-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try also one hot encoding\n",
    "if oneHotEncoderFlag:\n",
    "    X_train = pd.concat([obj_X_train, int_X_train], axis=1)\n",
    " \n",
    "    X_train = pd.DataFrame(data = imputer2.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    obj_X_train = X_train.drop(columns=int_X_train.columns)\n",
    "                               \n",
    "    obj_X_train_1hot = encoder.fit_transform(obj_X_train)\n",
    "    obj_X_test_1hot = encoder.transform(obj_X_test)\n",
    "                               \n",
    "    obj_X_train_1hot = obj_X_train_1hot.toarray()\n",
    "    obj_X_test_1hot = obj_X_test_1hot.toarray()\n",
    "                               \n",
    "    int_X_train = int_X_train.to_numpy()\n",
    "    int_X_test = int_X_test.to_numpy()\n",
    "    \n",
    "    # recreate X_train and X_test\n",
    "    X_train = np.hstack((obj_X_train_1hot,int_X_train))\n",
    "    X_test = np.hstack((obj_X_test_1hot,int_X_test))\n",
    "else:\n",
    "    # recreate X_train and X_test\n",
    "    X_train = pd.concat([obj_X_train, int_X_train], axis=1)\n",
    "    X_test = pd.concat([obj_X_test, int_X_test], axis=1)\n",
    "    X_train = pd.DataFrame(data = imputer2.fit_transform(X_train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smooth-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "pipe = Pipeline([('scale', StandardScaler()), ('clf', DecisionTreeClassifier(random_state=RANDOM_VARIABLE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sudden-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid search params\n",
    "grid_params = [{'clf__criterion': ['gini', 'entropy'],\n",
    "'clf__min_samples_leaf':  [5, 10, 15, 20, 25, 30, 40, 50],\n",
    "'clf__max_depth': [3, 5, 7, 8, 9, 10, 12, 15]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "musical-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct grid search\n",
    "cv = StratifiedKFold(n_splits=10shuffle=True, random_state=RANDOM_VARIABLE)\n",
    "gs = GridSearchCV(estimator=pipe,param_grid=grid_params,scoring='accuracy',cv=cv, n_jobs=2, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "assisted-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit using grid search\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best score (evaluation accuracy): {}'.format(gs.best_score_))\n",
    "print('Best parametes: {}'.format(gs.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mathematical-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "accScore = accuracy_score(y_test,y_pred)\n",
    "fScore = f1_score(y_test, y_pred)\n",
    "print(fScore)\n",
    "print(accScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "planned-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convinced-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "columns = ['age','fnlwgt','education_num','hours-per-week',\"capital-loss\",\"capital-gain\",\"income\"]\n",
    "data = pd.read_csv('income.csv',usecols=columns)\n",
    "data_test = pd.read_csv('income_test.csv',usecols=columns)\n",
    "# Convert target variable to 0 and 1\n",
    "data[\"income\"] = data[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "data_test[\"income\"] = data_test[\"income\"].map({ \"<=50K\": 0, \">50K\": 1 })\n",
    "# Create X and y\n",
    "X_train = data.drop([\"income\"],axis=1)\n",
    "y_train = data['income'].values\n",
    "X_test = data_test.drop([\"income\"],axis=1)\n",
    "y_test = data_test['income'].values\n",
    "# Classifier\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tired-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a learning curve for the classifer for the train and test set loaded above\n",
    "def learning_curve(classifier,X_train,y_train,X_test,y_test,train_sizes): \n",
    "    \n",
    "    acc_score_train = []\n",
    "    acc_score_test = []\n",
    "    for i in train_sizes:\n",
    "        \n",
    "        X_train_new = X_train.iloc[:i,:]\n",
    "        y_train_new = y_train[0:i]\n",
    "        \n",
    "        classifier.fit(X_train_new, y_train_new)\n",
    "        \n",
    "        y_pred_train = classifier.predict(X_train_new)\n",
    "        y_pred_test = classifier.predict(X_test)\n",
    "        \n",
    "        acc_score_train.append(accuracy_score(y_train_new, y_pred_train))\n",
    "        acc_score_test.append( accuracy_score(y_test, y_pred_test))\n",
    "        \n",
    "    # Plot learning curve    \n",
    "    plt.plot(train_sizes,acc_score_train, '-*', label=\"training set\")\n",
    "    plt.plot(train_sizes,acc_score_test, '-*', label=\"testing set\")\n",
    "    plt.ylabel('Accuracy scores')\n",
    "    plt.xlabel('Number of instances')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()   \n",
    "    \n",
    "    return acc_score_train, acc_score_test\n",
    "\n",
    "# Call learning curve\n",
    "lengthX = X_train.shape[0]\n",
    "train_sizes = np.linspace(50,lengthX,num=50).astype(int)\n",
    "acc_score_train, acc_score_test = learning_curve(classifier,X_train,y_train,X_test,y_test,train_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "respected-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations from the performance learning curve:\n",
    "\n",
    "# -Training score (accuracy) is at its maximum regardless of training examples. This shows severe overfitting.\n",
    "\n",
    "# -Testing score slightly increases.\n",
    "\n",
    "# -There is a huge gap between testing score and training score which indicates high variance scenario (again, this means overfitting).\n",
    "\n",
    "# Solutions - Dealing with overfitting: make the model not to fit as much to the data by changing the hyper parameters :\n",
    "\n",
    "# -Limiting the maximum depth or number of leaves of the tree\n",
    "\n",
    "# -Requiring a minimum number of points in a node to keep splitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ruled-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Accuracy and F1-score when implementing the initial decision tree model from 3.1 (DecisionTreeClassifier(min_samples_leaf=4))\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Initial Model\\n\")\n",
    "print(\"DecisionTreeClassifier scores with min_samples_leaf = 4:\")\n",
    "print(f\"Accuracy = {acc:.4f}\")\n",
    "print(f\"F1 = {f1:.4f}\\n\")\n",
    "print(f\"Tree max depth = {classifier.tree_.max_depth}\\n\")\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# Now let's try limit the maximum depth of the tree (keeping min_samples_leaf=4)\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "for d in range(1,15,1):\n",
    "    tree = DecisionTreeClassifier(max_depth=d, random_state=RANDOM_VARIABLE)\n",
    "    tree.fit(X_train, y_train)\n",
    "    acc_train = np.append(acc_train,tree.score(X_train, y_train))\n",
    "    acc_test = np.append(acc_test,tree.score(X_test, y_test))\n",
    "\n",
    "plt.figure(num=1)\n",
    "plt.plot(range(1,15,1),acc_train, '-*', label=\"training set\")\n",
    "plt.plot(range(1,15,1),acc_test, '-*', label=\"testing set\")\n",
    "plt.ylabel('Accuracy scores')\n",
    "plt.xlabel('Depth')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# As we can see from figure 1, the testing accuracy starts decreasing after max_depth = 8. So I will choose this value\n",
    "# for the max_depth parameter to see what will happen to the scores of the model.\n",
    "\n",
    "# Accuracy and F1-score of the decision tree model from 3.1 (DecisionTreeClassifier(min_samples_leaf=4)) with max_depth = 8\n",
    "classifier2 = DecisionTreeClassifier(max_depth=8, min_samples_leaf=4)\n",
    "classifier2.fit(X_train, y_train)\n",
    "y_pred = classifier2.predict(X_test)\n",
    "acc_2 = accuracy_score(y_test, y_pred)\n",
    "f1_2 = f1_score(y_test, y_pred)\n",
    "print(\"Second Model\\n\")\n",
    "print(\"DecisionTreeClassifier scores with min_samples_leaf = 4 and max_depth = 8:\")\n",
    "print(f\"Accuracy = {acc_2:.4f}\")\n",
    "print(f\"F1 = {f1_2:.4f}\\n\")\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# Requiring a minimum number of points in a node to keep splitting it\n",
    "accTrain = []\n",
    "accTest = []\n",
    "accVal = []\n",
    "lnS = []\n",
    "for ln in range(2,151,1):\n",
    "    lnS.append(ln)\n",
    "    tree = DecisionTreeClassifier(min_samples_leaf=ln, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    trainAc = tree.score(X_train, y_train)\n",
    "    accTrain.append(trainAc)\n",
    "    testAc = tree.score(X_test, y_test)\n",
    "    accTest.append(testAc)\n",
    "    \n",
    "plt.figure(num=2)    \n",
    "plt.plot(lnS,accTrain, label=\"training set\")\n",
    "plt.plot(lnS,accTest, label=\"test set\")\n",
    "plt.ylabel('Accuracy scores')\n",
    "plt.xlabel('Min samples leaf')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# As we can see from figure 2 the training and testing accuracies start slowly reaching a plateau at around 40-50\n",
    "# min_sample_leafs. Generally we want high training and testing accuracies without though an overfitting appearence. \n",
    "# So, based on the diagramm and on the above, I will try min_samples_leaf values 5 - 10.\n",
    "\n",
    "# Accuracy and F1-score when the decision tree model with min_samples_leaf = 7\n",
    "classifier3 = DecisionTreeClassifier(min_samples_leaf=7)\n",
    "classifier3.fit(X_train, y_train)\n",
    "y_pred = classifier3.predict(X_test)\n",
    "acc_3 = accuracy_score(y_test, y_pred)\n",
    "f1_3 = f1_score(y_test, y_pred)\n",
    "print(\"Third Model\")\n",
    "print(\"DecisionTreeClassifier scores with min_samples_leaf = 7:\")\n",
    "print(f\"Accuracy = {acc_3:.4f}\")\n",
    "print(f\"F1 = {f1_3:.4f}\\n\")\n",
    "\n",
    "# min_samples_leaf = 7 gave the best scores\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# Let's try max_depth = 8 and min_samples_leaf = 7\n",
    "classifier4 = DecisionTreeClassifier(max_depth=8, min_samples_leaf=7)\n",
    "classifier4.fit(X_train, y_train)\n",
    "y_pred = classifier4.predict(X_test)\n",
    "acc_4 = accuracy_score(y_test, y_pred)\n",
    "f1_4 = f1_score(y_test, y_pred)\n",
    "print(\"Fourth Model\")\n",
    "print(\"DecisionTreeClassifier scores max_depth=8 and with min_samples_leaf = 7:\")\n",
    "print(f\"Accuracy = {acc_4:.4f}\")\n",
    "print(f\"F1 = {f1_4:.4f}\\n\")\n",
    "# Results better from all the above\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# Let's try only limiting the max_depth to 8 and give no value to parameter min_samples_leaf\n",
    "classifier5 = DecisionTreeClassifier(max_depth=8)\n",
    "classifier5.fit(X_train, y_train)\n",
    "y_pred = classifier5.predict(X_test)\n",
    "acc_5 = accuracy_score(y_test, y_pred)\n",
    "f1_5 = f1_score(y_test, y_pred)\n",
    "print(\"Fifth Model\")\n",
    "print(\"DecisionTreeClassifier scores max_depth=8:\")\n",
    "print(f\"Accuracy = {acc_5:.4f}\")\n",
    "print(f\"F1 = {f1_5:.4f}\\n\")\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# So it's obvious that limiting the max_depth parameter and increasing a bit the min_samples_leaf parameter to 7 \n",
    "# gives the best scores.\n",
    "\n",
    "final_score = [acc_4,f1_4]\n",
    "print(\"Final Score\")\n",
    "print(final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
